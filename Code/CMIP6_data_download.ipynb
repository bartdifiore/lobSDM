{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data libraries and load the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       activity_id       institution_id      source_id       experiment_id  \\\n",
      "0       HighResMIP                 CMCC   CMCC-CM2-HR4  highresSST-present   \n",
      "1       HighResMIP                 CMCC   CMCC-CM2-HR4  highresSST-present   \n",
      "2       HighResMIP                 CMCC   CMCC-CM2-HR4  highresSST-present   \n",
      "3       HighResMIP                 CMCC   CMCC-CM2-HR4  highresSST-present   \n",
      "4       HighResMIP                 CMCC   CMCC-CM2-HR4  highresSST-present   \n",
      "...            ...                  ...            ...                 ...   \n",
      "514813        CMIP  EC-Earth-Consortium  EC-Earth3-Veg          historical   \n",
      "514814        CMIP  EC-Earth-Consortium  EC-Earth3-Veg          historical   \n",
      "514815        CMIP  EC-Earth-Consortium  EC-Earth3-Veg          historical   \n",
      "514816        CMIP  EC-Earth-Consortium  EC-Earth3-Veg          historical   \n",
      "514817        CMIP  EC-Earth-Consortium  EC-Earth3-Veg          historical   \n",
      "\n",
      "       member_id table_id variable_id grid_label  \\\n",
      "0       r1i1p1f1     Amon          ps         gn   \n",
      "1       r1i1p1f1     Amon        rsds         gn   \n",
      "2       r1i1p1f1     Amon        rlus         gn   \n",
      "3       r1i1p1f1     Amon        rlds         gn   \n",
      "4       r1i1p1f1     Amon         psl         gn   \n",
      "...          ...      ...         ...        ...   \n",
      "514813  r1i1p1f1     Amon         tas         gr   \n",
      "514814  r1i1p1f1     Amon        tauu         gr   \n",
      "514815  r1i1p1f1     Amon         hur         gr   \n",
      "514816  r1i1p1f1     Amon         hus         gr   \n",
      "514817  r1i1p1f1     Amon        tauv         gr   \n",
      "\n",
      "                                                   zstore  dcpp_init_year  \\\n",
      "0       gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...             NaN   \n",
      "1       gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...             NaN   \n",
      "2       gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...             NaN   \n",
      "3       gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...             NaN   \n",
      "4       gs://cmip6/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/...             NaN   \n",
      "...                                                   ...             ...   \n",
      "514813  gs://cmip6/CMIP6/CMIP/EC-Earth-Consortium/EC-E...             NaN   \n",
      "514814  gs://cmip6/CMIP6/CMIP/EC-Earth-Consortium/EC-E...             NaN   \n",
      "514815  gs://cmip6/CMIP6/CMIP/EC-Earth-Consortium/EC-E...             NaN   \n",
      "514816  gs://cmip6/CMIP6/CMIP/EC-Earth-Consortium/EC-E...             NaN   \n",
      "514817  gs://cmip6/CMIP6/CMIP/EC-Earth-Consortium/EC-E...             NaN   \n",
      "\n",
      "         version  \n",
      "0       20170706  \n",
      "1       20170706  \n",
      "2       20170706  \n",
      "3       20170706  \n",
      "4       20170706  \n",
      "...          ...  \n",
      "514813  20211207  \n",
      "514814  20211207  \n",
      "514815  20211207  \n",
      "514816  20211207  \n",
      "514817  20211207  \n",
      "\n",
      "[514818 rows x 11 columns]\n",
      "Index(['activity_id', 'institution_id', 'source_id', 'experiment_id',\n",
      "       'member_id', 'table_id', 'variable_id', 'grid_label', 'zstore',\n",
      "       'dcpp_init_year', 'version'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import intake\n",
    "import xarray as xr\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from packaging.version import Version\n",
    "import numpy as np\n",
    "import gcsfs\n",
    "import fsspec\n",
    "import xesmf as xe\n",
    "import json\n",
    "import sys\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from xmip.utils import google_cmip_col\n",
    "from xmip.preprocessing import combined_preprocessing\n",
    "\n",
    "# Load the CMIP6 catalog\n",
    "catalog_url = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\n",
    "# col = intake.open_esm_datastore(catalog_url)\n",
    "col = google_cmip_col()\n",
    "print(col.df)\n",
    "print(col.df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter for historical runs of models we already have projeciton information for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.zstore.dcpp_init_year.version'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='68' class='' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [68/68 00:42&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/preprocessing.py:96: UserWarning: CMIP.MIROC.MIROC6.historical.r1i1p1f1.Omon.gn.none.thetao:While renaming to target `lev_bounds`, more than one candidate was found ['lev_bnds', 'zlev_bnds']. Renaming lev_bnds to lev_bounds. Please double check results.\n",
      "  warnings.warn(\n",
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/preprocessing.py:96: UserWarning: CMIP.MIROC.MIROC6.historical.r1i1p1f1.Omon.gn.none.thetao:While renaming to target `lon_bounds`, more than one candidate was found ['x_bnds', 'vertices_longitude']. Renaming x_bnds to lon_bounds. Please double check results.\n",
      "  warnings.warn(\n",
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/preprocessing.py:96: UserWarning: CMIP.MIROC.MIROC6.historical.r1i1p1f1.Omon.gn.none.thetao:While renaming to target `lat_bounds`, more than one candidate was found ['y_bnds', 'vertices_latitude']. Renaming y_bnds to lat_bounds. Please double check results.\n",
      "  warnings.warn(\n",
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/preprocessing.py:229: UserWarning: CMIP.MIROC.MIROC6.historical.r1i1p1f1.Omon.gn.none.thetao: Unit correction failed with: Cannot convert variables:\n",
      "    incompatible units for variable 'lev': cannot convert a non-quantity using 'm' as unit\n",
      "  warnings.warn(\n",
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/preprocessing.py:96: UserWarning: CMIP.MIROC.MIROC6.historical.r1i1p1f1.Omon.gn.none.tos:While renaming to target `lon_bounds`, more than one candidate was found ['x_bnds', 'vertices_longitude']. Renaming x_bnds to lon_bounds. Please double check results.\n",
      "  warnings.warn(\n",
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/preprocessing.py:96: UserWarning: CMIP.MIROC.MIROC6.historical.r1i1p1f1.Omon.gn.none.tos:While renaming to target `lat_bounds`, more than one candidate was found ['y_bnds', 'vertices_latitude']. Renaming y_bnds to lat_bounds. Please double check results.\n",
      "  warnings.warn(\n",
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/preprocessing.py:96: UserWarning: CMIP.MRI.MRI-ESM2-0.historical.r1i1p1f1.Omon.gn.none.tos:While renaming to target `lon_bounds`, more than one candidate was found ['x_bnds', 'vertices_longitude']. Renaming x_bnds to lon_bounds. Please double check results.\n",
      "  warnings.warn(\n",
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/preprocessing.py:96: UserWarning: CMIP.MRI.MRI-ESM2-0.historical.r1i1p1f1.Omon.gn.none.tos:While renaming to target `lat_bounds`, more than one candidate was found ['y_bnds', 'vertices_latitude']. Renaming y_bnds to lat_bounds. Please double check results.\n",
      "  warnings.warn(\n",
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/preprocessing.py:96: UserWarning: CMIP.MRI.MRI-ESM2-0.historical.r1i1p1f1.Omon.gn.none.thetao:While renaming to target `lon_bounds`, more than one candidate was found ['x_bnds', 'vertices_longitude']. Renaming x_bnds to lon_bounds. Please double check results.\n",
      "  warnings.warn(\n",
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/preprocessing.py:96: UserWarning: CMIP.MRI.MRI-ESM2-0.historical.r1i1p1f1.Omon.gn.none.thetao:While renaming to target `lat_bounds`, more than one candidate was found ['y_bnds', 'vertices_latitude']. Renaming y_bnds to lat_bounds. Please double check results.\n",
      "  warnings.warn(\n",
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/preprocessing.py:287: UserWarning: `FGOALS-f3-L` does not provide lon or lat bounds.\n",
      "  warnings.warn(\"`FGOALS-f3-L` does not provide lon or lat bounds.\")\n",
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/preprocessing.py:287: UserWarning: `FGOALS-f3-L` does not provide lon or lat bounds.\n",
      "  warnings.warn(\"`FGOALS-f3-L` does not provide lon or lat bounds.\")\n",
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/preprocessing.py:229: UserWarning: CMIP.IPSL.IPSL-CM6A-LR.historical.r1i1p1f1.Omon.gn.none.thetao: Unit correction failed with: Cannot convert variables:\n",
      "    incompatible units for variable 'lev': cannot convert a non-quantity using 'm' as unit\n",
      "  warnings.warn(\n",
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/preprocessing.py:96: UserWarning: CMIP.NIMS-KMA.KACE-1-0-G.historical.r1i1p1f1.Omon.gr.none.tos:While renaming to target `lon_bounds`, more than one candidate was found ['lon_bnds', 'vertices_longitude']. Renaming lon_bnds to lon_bounds. Please double check results.\n",
      "  warnings.warn(\n",
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/preprocessing.py:96: UserWarning: CMIP.NIMS-KMA.KACE-1-0-G.historical.r1i1p1f1.Omon.gr.none.tos:While renaming to target `lat_bounds`, more than one candidate was found ['lat_bnds', 'vertices_latitude']. Renaming lat_bnds to lat_bounds. Please double check results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CMIP.BCC.BCC-CSM2-MR.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/BCC/BCC-CSM2-MR/historical/r1i1p1f1/Omon/tos/gn/v20181126/.20181126',\n",
       " 'CMIP.MIROC.MIROC6.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/MIROC/MIROC6/historical/r1i1p1f1/Omon/thetao/gn/v20190311/.20190311',\n",
       " 'CMIP.CSIRO-ARCCSS.ACCESS-CM2.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/CSIRO-ARCCSS/ACCESS-CM2/historical/r1i1p1f1/Omon/tos/gn/v20191108/.20191108',\n",
       " 'CMIP.EC-Earth-Consortium.EC-Earth3-Veg-LR.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/EC-Earth-Consortium/EC-Earth3-Veg-LR/historical/r1i1p1f1/Omon/thetao/gn/v20200919/.20200919',\n",
       " 'CMIP.NUIST.NESM3.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/NUIST/NESM3/historical/r1i1p1f1/Omon/thetao/gn/v20190703/.20190703',\n",
       " 'CMIP.MIROC.MIROC6.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/MIROC/MIROC6/historical/r1i1p1f1/Omon/tos/gn/v20181212/.20181212',\n",
       " 'CMIP.EC-Earth-Consortium.EC-Earth3-Veg-LR.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/EC-Earth-Consortium/EC-Earth3-Veg-LR/historical/r1i1p1f1/Omon/tos/gn/v20200919/.20200919',\n",
       " 'CMIP.EC-Earth-Consortium.EC-Earth3.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/EC-Earth-Consortium/EC-Earth3/historical/r1i1p1f1/Omon/thetao/gn/v20200918/.20200918',\n",
       " 'CMIP.CMCC.CMCC-ESM2.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/CMCC/CMCC-ESM2/historical/r1i1p1f1/Omon/thetao/gn/v20210114/.20210114',\n",
       " 'CMIP.INM.INM-CM4-8.historical.r1i1p1f1.Omon.tos.gr1.gs://cmip6/CMIP6/CMIP/INM/INM-CM4-8/historical/r1i1p1f1/Omon/tos/gr1/v20190530/.20190530',\n",
       " 'CMIP.THU.CIESM.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/THU/CIESM/historical/r1i1p1f1/Omon/tos/gn/v20200220/.20200220',\n",
       " 'CMIP.NCAR.CESM2-WACCM.historical.r1i1p1f1.Omon.thetao.gr.gs://cmip6/CMIP6/CMIP/NCAR/CESM2-WACCM/historical/r1i1p1f1/Omon/thetao/gr/v20190808/.20190808',\n",
       " 'CMIP.EC-Earth-Consortium.EC-Earth3.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/EC-Earth-Consortium/EC-Earth3/historical/r1i1p1f1/Omon/tos/gn/v20200918/.20200918',\n",
       " 'CMIP.FIO-QLNM.FIO-ESM-2-0.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/FIO-QLNM/FIO-ESM-2-0/historical/r1i1p1f1/Omon/tos/gn/v20191122/.20191122',\n",
       " 'CMIP.NCAR.CESM2-WACCM.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/NCAR/CESM2-WACCM/historical/r1i1p1f1/Omon/tos/gn/v20190808/.20190808',\n",
       " 'CMIP.CCCma.CanESM5.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/CCCma/CanESM5/historical/r1i1p1f1/Omon/tos/gn/v20190429/.20190429',\n",
       " 'CMIP.CSIRO-ARCCSS.ACCESS-CM2.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/CSIRO-ARCCSS/ACCESS-CM2/historical/r1i1p1f1/Omon/thetao/gn/v20191108/.20191108',\n",
       " 'CMIP.NOAA-GFDL.GFDL-ESM4.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-ESM4/historical/r1i1p1f1/Omon/thetao/gn/v20190726/.20190726',\n",
       " 'CMIP.MPI-M.MPI-ESM1-2-HR.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/MPI-M/MPI-ESM1-2-HR/historical/r1i1p1f1/Omon/tos/gn/v20190710/.20190710',\n",
       " 'CMIP.NCC.NorESM2-MM.historical.r1i1p1f1.Omon.thetao.gr.gs://cmip6/CMIP6/CMIP/NCC/NorESM2-MM/historical/r1i1p1f1/Omon/thetao/gr/v20191108/.20191108',\n",
       " 'CMIP.MRI.MRI-ESM2-0.historical.r1i1p1f1.Omon.tos.gr.gs://cmip6/CMIP6/CMIP/MRI/MRI-ESM2-0/historical/r1i1p1f1/Omon/tos/gr/v20190904/.20190904',\n",
       " 'CMIP.NOAA-GFDL.GFDL-ESM4.historical.r1i1p1f1.Omon.tos.gr.gs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-ESM4/historical/r1i1p1f1/Omon/tos/gr/v20190726/.20190726',\n",
       " 'CMIP.UA.MCM-UA-1-0.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/UA/MCM-UA-1-0/historical/r1i1p1f1/Omon/tos/gn/v20190731/.20190731',\n",
       " 'CMIP.NUIST.NESM3.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/NUIST/NESM3/historical/r1i1p1f1/Omon/tos/gn/v20190703/.20190703',\n",
       " 'CMIP.CCCma.CanESM5.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/CCCma/CanESM5/historical/r1i1p1f1/Omon/thetao/gn/v20190429/.20190429',\n",
       " 'CMIP.INM.INM-CM5-0.historical.r1i1p1f1.Omon.tos.gr1.gs://cmip6/CMIP6/CMIP/INM/INM-CM5-0/historical/r1i1p1f1/Omon/tos/gr1/v20190610/.20190610',\n",
       " 'CMIP.MRI.MRI-ESM2-0.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/MRI/MRI-ESM2-0/historical/r1i1p1f1/Omon/tos/gn/v20190904/.20190904',\n",
       " 'CMIP.MRI.MRI-ESM2-0.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/MRI/MRI-ESM2-0/historical/r1i1p1f1/Omon/thetao/gn/v20210311/.20210311',\n",
       " 'CMIP.NOAA-GFDL.GFDL-ESM4.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-ESM4/historical/r1i1p1f1/Omon/tos/gn/v20190726/.20190726',\n",
       " 'CMIP.CAS.FGOALS-g3.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/CAS/FGOALS-g3/historical/r1i1p1f1/Omon/thetao/gn/v20191012/.20191012',\n",
       " 'CMIP.THU.CIESM.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/THU/CIESM/historical/r1i1p1f1/Omon/thetao/gn/v20200417/.20200417',\n",
       " 'CMIP.CAS.FGOALS-f3-L.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/CAS/FGOALS-f3-L/historical/r1i1p1f1/Omon/tos/gn/v20191007/.20191007',\n",
       " 'CMIP.CSIRO.ACCESS-ESM1-5.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/CSIRO/ACCESS-ESM1-5/historical/r1i1p1f1/Omon/thetao/gn/v20191115/.20191115',\n",
       " 'CMIP.NCAR.CESM2.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/NCAR/CESM2/historical/r1i1p1f1/Omon/tos/gn/v20190308/.20190308',\n",
       " 'CMIP.CAS.FGOALS-f3-L.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/CAS/FGOALS-f3-L/historical/r1i1p1f1/Omon/thetao/gn/v20191007/.20191007',\n",
       " 'CMIP.MPI-M.MPI-ESM1-2-HR.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/MPI-M/MPI-ESM1-2-HR/historical/r1i1p1f1/Omon/thetao/gn/v20190710/.20190710',\n",
       " 'CMIP.MRI.MRI-ESM2-0.historical.r1i1p1f1.Omon.thetao.gr.gs://cmip6/CMIP6/CMIP/MRI/MRI-ESM2-0/historical/r1i1p1f1/Omon/thetao/gr/v20191205/.20191205',\n",
       " 'CMIP.CCCR-IITM.IITM-ESM.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/CCCR-IITM/IITM-ESM/historical/r1i1p1f1/Omon/tos/gn/v20200915/.20200915',\n",
       " 'CMIP.NASA-GISS.GISS-E2-1-G.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/NASA-GISS/GISS-E2-1-G/historical/r1i1p1f1/Omon/tos/gn/v20180827/.20180827',\n",
       " 'CMIP.CAS.FGOALS-g3.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/CAS/FGOALS-g3/historical/r1i1p1f1/Omon/tos/gn/v20191107/.20191107',\n",
       " 'CMIP.NASA-GISS.GISS-E2-1-G.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/NASA-GISS/GISS-E2-1-G/historical/r1i1p1f1/Omon/thetao/gn/v20180827/.20180827',\n",
       " 'CMIP.CMCC.CMCC-CM2-SR5.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/CMCC/CMCC-CM2-SR5/historical/r1i1p1f1/Omon/tos/gn/v20200616/.20200616',\n",
       " 'CMIP.INM.INM-CM5-0.historical.r1i1p1f1.Omon.thetao.gr1.gs://cmip6/CMIP6/CMIP/INM/INM-CM5-0/historical/r1i1p1f1/Omon/thetao/gr1/v20190610/.20190610',\n",
       " 'CMIP.NCC.NorESM2-LM.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/NCC/NorESM2-LM/historical/r1i1p1f1/Omon/thetao/gn/v20190815/.20190815',\n",
       " 'CMIP.NCAR.CESM2-WACCM.historical.r1i1p1f1.Omon.tos.gr.gs://cmip6/CMIP6/CMIP/NCAR/CESM2-WACCM/historical/r1i1p1f1/Omon/tos/gr/v20190808/.20190808',\n",
       " 'CMIP.MPI-M.MPI-ESM1-2-LR.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/MPI-M/MPI-ESM1-2-LR/historical/r1i1p1f1/Omon/tos/gn/v20190710/.20190710',\n",
       " 'CMIP.NCC.NorESM2-LM.historical.r1i1p1f1.Omon.thetao.gr.gs://cmip6/CMIP6/CMIP/NCC/NorESM2-LM/historical/r1i1p1f1/Omon/thetao/gr/v20190815/.20190815',\n",
       " 'CMIP.NCC.NorESM2-MM.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/NCC/NorESM2-MM/historical/r1i1p1f1/Omon/tos/gn/v20191108/.20191108',\n",
       " 'CMIP.CAMS.CAMS-CSM1-0.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/CAMS/CAMS-CSM1-0/historical/r1i1p1f1/Omon/thetao/gn/v20190708/.20190708',\n",
       " 'CMIP.CMCC.CMCC-CM2-SR5.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/CMCC/CMCC-CM2-SR5/historical/r1i1p1f1/Omon/thetao/gn/v20200616/.20200616',\n",
       " 'CMIP.NCAR.CESM2.historical.r1i1p1f1.Omon.tos.gr.gs://cmip6/CMIP6/CMIP/NCAR/CESM2/historical/r1i1p1f1/Omon/tos/gr/v20190308/.20190308',\n",
       " 'CMIP.UA.MCM-UA-1-0.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/UA/MCM-UA-1-0/historical/r1i1p1f1/Omon/thetao/gn/v20190731/.20190731',\n",
       " 'CMIP.IPSL.IPSL-CM6A-LR.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/IPSL/IPSL-CM6A-LR/historical/r1i1p1f1/Omon/tos/gn/v20180803/.20180803',\n",
       " 'CMIP.IPSL.IPSL-CM6A-LR.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/IPSL/IPSL-CM6A-LR/historical/r1i1p1f1/Omon/thetao/gn/v20180803/.20180803',\n",
       " 'CMIP.NCC.NorESM2-MM.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/NCC/NorESM2-MM/historical/r1i1p1f1/Omon/thetao/gn/v20191108/.20191108',\n",
       " 'CMIP.MPI-M.MPI-ESM1-2-LR.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/MPI-M/MPI-ESM1-2-LR/historical/r1i1p1f1/Omon/thetao/gn/v20190710/.20190710',\n",
       " 'CMIP.CAMS.CAMS-CSM1-0.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/CAMS/CAMS-CSM1-0/historical/r1i1p1f1/Omon/tos/gn/v20190708/.20190708',\n",
       " 'CMIP.BCC.BCC-CSM2-MR.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/BCC/BCC-CSM2-MR/historical/r1i1p1f1/Omon/thetao/gn/v20181126/.20181126',\n",
       " 'CMIP.INM.INM-CM4-8.historical.r1i1p1f1.Omon.thetao.gr1.gs://cmip6/CMIP6/CMIP/INM/INM-CM4-8/historical/r1i1p1f1/Omon/thetao/gr1/v20190530/.20190530',\n",
       " 'CMIP.CSIRO.ACCESS-ESM1-5.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/CSIRO/ACCESS-ESM1-5/historical/r1i1p1f1/Omon/tos/gn/v20191115/.20191115',\n",
       " 'CMIP.CMCC.CMCC-ESM2.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/CMCC/CMCC-ESM2/historical/r1i1p1f1/Omon/tos/gn/v20210114/.20210114',\n",
       " 'CMIP.NCC.NorESM2-LM.historical.r1i1p1f1.Omon.tos.gn.gs://cmip6/CMIP6/CMIP/NCC/NorESM2-LM/historical/r1i1p1f1/Omon/tos/gn/v20190815/.20190815',\n",
       " 'CMIP.NOAA-GFDL.GFDL-ESM4.historical.r1i1p1f1.Omon.thetao.gr.gs://cmip6/CMIP6/CMIP/NOAA-GFDL/GFDL-ESM4/historical/r1i1p1f1/Omon/thetao/gr/v20190726/.20190726',\n",
       " 'CMIP.NIMS-KMA.KACE-1-0-G.historical.r1i1p1f1.Omon.tos.gr.gs://cmip6/CMIP6/CMIP/NIMS-KMA/KACE-1-0-G/historical/r1i1p1f1/Omon/tos/gr/v20200130/.20200130',\n",
       " 'CMIP.NCAR.CESM2-WACCM.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/NCAR/CESM2-WACCM/historical/r1i1p1f1/Omon/thetao/gn/v20190808/.20190808',\n",
       " 'CMIP.FIO-QLNM.FIO-ESM-2-0.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/FIO-QLNM/FIO-ESM-2-0/historical/r1i1p1f1/Omon/thetao/gn/v20191125/.20191125',\n",
       " 'CMIP.NCAR.CESM2.historical.r1i1p1f1.Omon.thetao.gn.gs://cmip6/CMIP6/CMIP/NCAR/CESM2/historical/r1i1p1f1/Omon/thetao/gn/v20190308/.20190308',\n",
       " 'CMIP.NCAR.CESM2.historical.r1i1p1f1.Omon.thetao.gr.gs://cmip6/CMIP6/CMIP/NCAR/CESM2/historical/r1i1p1f1/Omon/thetao/gr/v20190308/.20190308']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the folder containing your existing CMIP6 models\n",
    "# model_folder_path = \"/Users/aallyn/Library/CloudStorage/Box-Box/RES_Data/CMIP6/SSP5_85/BiasCorrected/IndividualModels/surf_temp\"  # replace with your folder path\n",
    "\n",
    "# # Get list of model names from folder\n",
    "# strings_to_remove = [\"surf_temp_stGrid_tos_\", \".grd\"]  # replace with actual strings to remove\n",
    "\n",
    "# ssp585_models = []\n",
    "# for name in os.listdir(model_folder_path):\n",
    "#     # Only include specific file types if needed (e.g., .nc files)\n",
    "#     if name.endswith(\"historical.grd\"):  # Optional: Filter by file extension\n",
    "#         # Remove each unwanted string from the file name\n",
    "#         clean_name = name\n",
    "#         for string in strings_to_remove:\n",
    "#             clean_name = clean_name.replace(string, \"\")\n",
    "#         ssp585_models.append(clean_name)\n",
    "# print(\"List of model names:\", ssp585_models)\n",
    "# print(col.df['source_id'].unique())\n",
    "\n",
    "# # Filter for historical data of the specified models\n",
    "# # Extract the first and second parts\n",
    "# # Split each model name by '_'\n",
    "# split_names = [name.split('_') for name in ssp585_models]\n",
    "\n",
    "# Extract just the IDs (the last element in the split parts)\n",
    "# source_ids = [parts[0] for parts in split_names]\n",
    "# Get info from the input json file, which was largely taken from NOAA CMIP6 data portal\n",
    "# Path to your JSON file\n",
    "file_path = \"/Users/aallyn/GitHub/lobSDM/Code/cmip6_input.json\"\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(file_path, \"r\") as file:\n",
    "    input = json.load(file)\n",
    "\n",
    "data_set = input['dataset']\n",
    "source_ids = {item[\"source_id\"] for item in data_set.values()}\n",
    "member_ids = {member for item in data_set.values() for member in item[\"ens_members\"]}\n",
    "variable_ids = [\"thetao\", \"tos\"]\n",
    "table_ids = [\"Omon\"]\n",
    "\n",
    "cat = col.search(\n",
    "    experiment_id = 'historical',\n",
    "    source_id = source_ids,\n",
    "    member_id = member_ids,\n",
    "    variable_id = variable_ids,\n",
    "    table_id = table_ids) # Filter for the specific models and variants\n",
    "\n",
    "# kwargs for combined pre_processing\n",
    "kwargs = {\n",
    "    'zarr_kwargs':{\n",
    "        'consolidated':True,\n",
    "        'use_cftime':True\n",
    "    },\n",
    "    'aggregate':False,\n",
    "    'preprocess':combined_preprocessing\n",
    "}\n",
    "\n",
    "ddict = cat.to_dataset_dict(\n",
    "    zarr_kwargs={\"consolidated\": True, \"use_cftime\": True},\n",
    "    aggregate=False,\n",
    "    preprocess=combined_preprocessing,\n",
    ")\n",
    "list(ddict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regrid models to common 1 x 1 grid\n",
    "\n",
    "It looks like there are a variety of different ways to do this. For simplicity, going to try to use one of the gr1 grids and keep going with the Pangeo CMIP6 tutorial workflow rather than departing and doing custom `xesmf` regridding. I think we can use 'CMIP.INM.INM-CM4-8.historical.r1i1p1f1.Omon.thetao.gr1.gs://cmip6/CMIP6/CMIP/INM/INM-CM4-8/historical/r1i1p1f1/Omon/thetao/gr1/v20190530/.20190530' as a template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/postprocessing.py:157: UserWarning: CMIP.NCAR.CESM2-WACCM.historical.r1i1p1f1.Omon.gr.none.thetao failed to combine with :Could not find any variable with the target_grid_label (gr1). Found these instead: ['gn' 'gr']\n",
      "  warnings.warn(f\"{cmip6_dataset_id(ds)} failed to combine with :{e}\")\n",
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/postprocessing.py:157: UserWarning: CMIP.NOAA-GFDL.GFDL-ESM4.historical.r1i1p1f1.Omon.gn.none.thetao failed to combine with :Could not find any variable with the target_grid_label (gr1). Found these instead: ['gn' 'gr']\n",
      "  warnings.warn(f\"{cmip6_dataset_id(ds)} failed to combine with :{e}\")\n",
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/postprocessing.py:157: UserWarning: CMIP.NCC.NorESM2-MM.historical.r1i1p1f1.Omon.gr.none.thetao failed to combine with :Could not find any variable with the target_grid_label (gr1). Found these instead: ['gn' 'gr']\n",
      "  warnings.warn(f\"{cmip6_dataset_id(ds)} failed to combine with :{e}\")\n",
      "/Users/aallyn/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/postprocessing.py:157: UserWarning: CMIP.MRI.MRI-ESM2-0.historical.r1i1p1f1.Omon.gr.none.tos failed to combine with :Could not find any variable with the target_grid_label (gr1). Found these instead: ['gn' 'gr']\n",
      "  warnings.warn(f\"{cmip6_dataset_id(ds)} failed to combine with :{e}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['BCC-CSM2-MR.historical.Omon.r1i1p1f1',\n",
       " 'MIROC6.historical.Omon.r1i1p1f1',\n",
       " 'ACCESS-CM2.historical.Omon.r1i1p1f1',\n",
       " 'EC-Earth3-Veg-LR.historical.Omon.r1i1p1f1',\n",
       " 'NESM3.historical.Omon.r1i1p1f1',\n",
       " 'EC-Earth3.historical.Omon.r1i1p1f1',\n",
       " 'CMCC-ESM2.historical.Omon.r1i1p1f1',\n",
       " 'INM-CM4-8.historical.Omon.r1i1p1f1',\n",
       " 'CIESM.historical.Omon.r1i1p1f1',\n",
       " 'FIO-ESM-2-0.historical.Omon.r1i1p1f1',\n",
       " 'CanESM5.historical.Omon.r1i1p1f1',\n",
       " 'MPI-ESM1-2-HR.historical.Omon.r1i1p1f1',\n",
       " 'MCM-UA-1-0.historical.Omon.r1i1p1f1',\n",
       " 'INM-CM5-0.historical.Omon.r1i1p1f1',\n",
       " 'FGOALS-g3.historical.Omon.r1i1p1f1',\n",
       " 'FGOALS-f3-L.historical.Omon.r1i1p1f1',\n",
       " 'ACCESS-ESM1-5.historical.Omon.r1i1p1f1',\n",
       " 'CESM2.historical.Omon.r1i1p1f1',\n",
       " 'IITM-ESM.historical.Omon.r1i1p1f1',\n",
       " 'GISS-E2-1-G.historical.Omon.r1i1p1f1',\n",
       " 'CMCC-CM2-SR5.historical.Omon.r1i1p1f1',\n",
       " 'NorESM2-LM.historical.Omon.r1i1p1f1',\n",
       " 'MPI-ESM1-2-LR.historical.Omon.r1i1p1f1',\n",
       " 'CAMS-CSM1-0.historical.Omon.r1i1p1f1',\n",
       " 'IPSL-CM6A-LR.historical.Omon.r1i1p1f1',\n",
       " 'KACE-1-0-G.historical.Omon.r1i1p1f1']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xmip.postprocessing import (\n",
    "    interpolate_grid_label\n",
    ")\n",
    "\n",
    "combined_grids_dict = interpolate_grid_label(ddict, target_grid_label='gr1')\n",
    "list(combined_grids_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0minterpolate_grid_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mds_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtarget_grid_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gn'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mxesmf_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'ignore_degenerate'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'periodic'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmerge_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'combine_attrs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'drop_conflicts'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Combines different grid labels via interpolation with xesmf\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "ds_dict : dict\n",
      "    dictonary of input datasets\n",
      "target_grid_label : str, optional\n",
      "    preferred grid_label value. If at least one dataset has this grid_label, otherse are interpolated to it.\n",
      "    Dataset with this grid label are not modified, by default \"gn\"\n",
      "method : str, optional\n",
      "    interpolation method for xesmf, by default \"bilinear\"\n",
      "xesmf_kwargs : dict, optional\n",
      "    optional arguments for building xesmf regridder, by default {}\n",
      "merge_kwargs : dict, optional\n",
      "    optional arguments for the merging of interpolated datasets, by default {}\n",
      "verbose : bool, optional\n",
      "    print output while creating regridder, by default False\n",
      "\n",
      "Returns\n",
      "-------\n",
      "dict\n",
      "    dictionary of combined datasets (usually will combine across different variable ids)\n",
      "\u001b[0;31mFile:\u001b[0m      ~/GitHub/lobSDM/.pixi/envs/default/lib/python3.10/site-packages/xmip/postprocessing.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "interpolate_grid_label?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and save the historical data for region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice(20.54499053955078, 82.37899780273438, None)\n",
      "slice(254.0767822265625, 323.5, None)\n"
     ]
    }
   ],
   "source": [
    "# Define your region of interest (extent)\n",
    "# Load the NetCDF file using xarray\n",
    "ds = xr.open_dataset(\"/Users/aallyn/Library/CloudStorage/Box-Box/RES_Data/CMIP6/SSP1_26/RawTmpFiles/so_CanESM5_r10i1p1f1_ssp126.nc\")\n",
    "\n",
    "# Get the bounds (assuming the latitudes and longitudes are called 'lat' and 'lon')\n",
    "lat_bounds = slice(ds['latitude'].values.min(), ds['latitude'].values.max())\n",
    "print(lat_bounds)\n",
    "lon_bounds = slice(ds['longitude'].values.min(), ds['longitude'].values.max())\n",
    "print(lon_bounds)\n",
    "\n",
    "# Download\n",
    "box_root = \"/Users/aallyn/Library/CloudStorage/Box-Box/RES_Data/CMIP6/SSP5_85/RawTmpFiles/\"\n",
    " \n",
    "# # I couldn't get this to work!       \n",
    "# for key, ds in dset_dict.items():\n",
    "#     # print(f\"Model: {key}, Type of dataset: {type(ds)}\")\n",
    "#     file_name = f\"{box_root}{key}.nc\"\n",
    "#     try:\n",
    "#         print(f\"Processing {key}...\")\n",
    "\n",
    "        # # Dynamically find longitude and latitude coordinates\n",
    "        # lon_name = None\n",
    "        # lat_name = None\n",
    "        # for name in ds.coords:\n",
    "        #     if 'lon' in name.lower() or 'longitude' in name.lower():\n",
    "        #         lon_name = name\n",
    "        #     if 'lat' in name.lower() or 'latitude' in name.lower():\n",
    "        #         lat_name = name\n",
    "\n",
    "        # # Check if lon/lat coordinates were found\n",
    "        # if lon_name is None or lat_name is None:\n",
    "        #     # If not, check for j/i indexing\n",
    "        #     if 'j' in ds.coords and 'i' in ds.coords:\n",
    "        #         print(\"Using j/i indexing instead of lat/lon.\")\n",
    "        #         lon_name = 'i'  # Use 'i' for longitude\n",
    "        #         lat_name = 'j'  # Use 'j' for latitude\n",
    "        #     else:\n",
    "        #         raise ValueError(\"No longitude or latitude coordinates found in the dataset.\")\n",
    "\n",
    "        # # Subset region based on identified coordinates\n",
    "        # if lon_name in ['i', 'j']:  # If using i/j indexing instead of lon/lat\n",
    "        #     subset_ds = ds.isel(\n",
    "        #         **{\n",
    "        #             lon_name: slice(lon_bounds[0], lon_bounds[1]),\n",
    "        #             lat_name: slice(lat_bounds[0], lat_bounds[1]),\n",
    "        #         }\n",
    "        #     )\n",
    "        # else:\n",
    "        #     subset_ds = ds.sel(\n",
    "        #         **{\n",
    "        #             lon_name: slice(lon_bounds[0], lon_bounds[1]),\n",
    "        #             lat_name: slice(lat_bounds[0], lat_bounds[1]),\n",
    "        #         }\n",
    "        #     )\n",
    "        \n",
    "        # # Fix encoding for time coordinate\n",
    "        # # Extract the time encoding and units\n",
    "        # if 'time' in subset_ds.coords:\n",
    "        #     original_time_encoding = ds['time'].encoding\n",
    "        #     units = original_time_encoding.get('units', 'days since 1850-01-01')\n",
    "        #     dtype = original_time_encoding.get('dtype', 'float64')  # Ensure dtype is set\n",
    "\n",
    "        #     # If dtype is None, set it explicitly to 'float64'\n",
    "        #     if dtype is None:\n",
    "        #         dtype = 'float64'\n",
    "\n",
    "        #     # Set time encoding to match units and dtype\n",
    "        #     subset_ds['time'].encoding.update({\n",
    "        #         'units': units,\n",
    "        #         'dtype': dtype,\n",
    "        #         '_FillValue': None,  # Avoid conflicts with fill values\n",
    "        #     })\n",
    "            \n",
    "        #     # If time bounds exist, set units for time_bnds as well\n",
    "        #     if 'time_bnds' in subset_ds.coords:\n",
    "        #         subset_ds['time_bnds'].encoding.update({\n",
    "        #             'units': units,  # Match the units of time\n",
    "        #         })\n",
    "                \n",
    "        #     # Remove chunking to prevent issues with datetime encoding\n",
    "        #     subset_ds['time'].encoding['chunks'] = None\n",
    "                \n",
    "        # # Save the subset file\n",
    "        # subset_ds.to_netcdf(file_name, encoding={var: {} for var in subset_ds.variables})\n",
    "        # print(f\"Saved subset file for {key} to {file_name}\")\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error processing {key}: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New option from pangeo example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using metadta from https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv\n",
      "MIROC6\n",
      "{'source_id': 'MIROC6', 'ens_members': ['r1i1p1f1']}\n",
      "Trying to download: \n",
      "table_id == 'Omon' & variable_id == 'tos' & experiment_id == 'historical' & source_id == 'MIROC6' & member_id == 'r1i1p1f1'\n",
      "Regridding irregular grid\n",
      "<xarray.Dataset>\n",
      "Dimensions:             (time: 1980, lat: 180, lon: 360, vertices: 4, bnds: 2)\n",
      "Coordinates:\n",
      "  * time                (time) datetime64[ns] 1850-01-16T12:00:00 ... 2014-12...\n",
      "    time_bnds           (time, bnds) datetime64[ns] dask.array<chunksize=(1980, 2), meta=np.ndarray>\n",
      "  * lat                 (lat) float64 -89.5 -88.5 -87.5 -86.5 ... 87.5 88.5 89.5\n",
      "  * lon                 (lon) float64 0.5 1.5 2.5 3.5 ... 357.5 358.5 359.5\n",
      "Dimensions without coordinates: vertices, bnds\n",
      "Data variables:\n",
      "    tos                 (time, lat, lon) float32 dask.array<chunksize=(312, 180, 360), meta=np.ndarray>\n",
      "    vertices_latitude   (vertices, lat, lon) float32 dask.array<chunksize=(4, 180, 360), meta=np.ndarray>\n",
      "    vertices_longitude  (vertices, lat, lon) float32 dask.array<chunksize=(4, 180, 360), meta=np.ndarray>\n",
      "Attributes:\n",
      "    regrid_method:  bilinear\n",
      "Done subsetting regular grid\n",
      "saved regridded data:  /Users/aallyn/Library/CloudStorage/Box-Box/RES_Data/CMIP6/SSP5_85/RawTmpFiles/tos_historical_Omon_MIROC6_r1i1p1f1_1850_2014_1x1.nc\n",
      "---------------------------------------\n",
      "MIROC-ES2L\n",
      "{'source_id': 'MIROC-ES2L', 'ens_members': ['r1i1p1f1']}\n",
      "Data not found for: tos\n",
      "table_id == 'Omon' & variable_id == 'tos' & experiment_id == 'historical' & source_id == 'MIROC-ES2L' & member_id == 'r1i1p1f1'\n",
      "---------------------------------------\n",
      "MCM-UA-1-0\n",
      "{'source_id': 'MCM-UA-1-0', 'ens_members': ['r1i1p1f1']}\n",
      "Trying to download: \n",
      "table_id == 'Omon' & variable_id == 'tos' & experiment_id == 'historical' & source_id == 'MCM-UA-1-0' & member_id == 'r1i1p1f1'\n",
      "Regridding irregular grid\n",
      "<xarray.Dataset>\n",
      "Dimensions:    (time: 1980, lat: 180, lon: 360, bnds: 2)\n",
      "Coordinates:\n",
      "  * bnds       (bnds) float64 1.0 2.0\n",
      "  * time       (time) object 1850-01-17 00:00:00 ... 2014-12-17 00:00:00\n",
      "    time_bnds  (time, bnds) object dask.array<chunksize=(1980, 2), meta=np.ndarray>\n",
      "  * lat        (lat) float64 -89.5 -88.5 -87.5 -86.5 ... 86.5 87.5 88.5 89.5\n",
      "  * lon        (lon) float64 0.5 1.5 2.5 3.5 4.5 ... 356.5 357.5 358.5 359.5\n",
      "Data variables:\n",
      "    tos        (time, lat, lon) float32 dask.array<chunksize=(1626, 180, 360), meta=np.ndarray>\n",
      "Attributes:\n",
      "    regrid_method:  bilinear\n",
      "Done subsetting regular grid\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable 'time_bnds' has conflicting _FillValue (nan) and missing_value (1.0000000200408773e+20). Cannot encode data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 299\u001b[0m\n\u001b[1;32m    297\u001b[0m file_name_nc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{var}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{exp}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{tab}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{sroc}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{vrnt}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{syr}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{eyr}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{t_lat}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m{t_lon}\u001b[39;00m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(var\u001b[38;5;241m=\u001b[39mvariable, exp\u001b[38;5;241m=\u001b[39mexperiment, tab\u001b[38;5;241m=\u001b[39mtable, sroc\u001b[38;5;241m=\u001b[39msrc, vrnt\u001b[38;5;241m=\u001b[39mvariant, syr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(start_year), eyr\u001b[38;5;241m=\u001b[39mend_year, t_lat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(tar_lat_int), t_lon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(tar_lon_int))\n\u001b[1;32m    298\u001b[0m out_file_out \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_dir, file_name_nc_out)\n\u001b[0;32m--> 299\u001b[0m ds_out\u001b[38;5;241m.\u001b[39mto_netcdf(out_file_out)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# close dataset\u001b[39;00m\n\u001b[1;32m    302\u001b[0m ds_out\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/GitHub/lobSDM/.conda/lib/python3.11/site-packages/xarray/core/dataset.py:1957\u001b[0m, in \u001b[0;36mDataset.to_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1954\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1955\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_netcdf\n\u001b[0;32m-> 1957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_netcdf(  \u001b[38;5;66;03m# type: ignore  # mypy cannot resolve the overloads:(\u001b[39;00m\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1959\u001b[0m     path,\n\u001b[1;32m   1960\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m   1961\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[1;32m   1962\u001b[0m     group\u001b[38;5;241m=\u001b[39mgroup,\n\u001b[1;32m   1963\u001b[0m     engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[1;32m   1964\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   1965\u001b[0m     unlimited_dims\u001b[38;5;241m=\u001b[39munlimited_dims,\n\u001b[1;32m   1966\u001b[0m     compute\u001b[38;5;241m=\u001b[39mcompute,\n\u001b[1;32m   1967\u001b[0m     multifile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1968\u001b[0m     invalid_netcdf\u001b[38;5;241m=\u001b[39minvalid_netcdf,\n\u001b[1;32m   1969\u001b[0m )\n",
      "File \u001b[0;32m~/GitHub/lobSDM/.conda/lib/python3.11/site-packages/xarray/backends/api.py:1272\u001b[0m, in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;66;03m# TODO: figure out how to refactor this logic (here and in save_mfdataset)\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;66;03m# to avoid this mess of conditionals\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;66;03m# TODO: allow this work (setting up the file for writing array data)\u001b[39;00m\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;66;03m# to be parallelized with dask\u001b[39;00m\n\u001b[0;32m-> 1272\u001b[0m     dump_to_store(\n\u001b[1;32m   1273\u001b[0m         dataset, store, writer, encoding\u001b[38;5;241m=\u001b[39mencoding, unlimited_dims\u001b[38;5;241m=\u001b[39munlimited_dims\n\u001b[1;32m   1274\u001b[0m     )\n\u001b[1;32m   1275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m autoclose:\n\u001b[1;32m   1276\u001b[0m         store\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/GitHub/lobSDM/.conda/lib/python3.11/site-packages/xarray/backends/api.py:1319\u001b[0m, in \u001b[0;36mdump_to_store\u001b[0;34m(dataset, store, writer, encoder, encoding, unlimited_dims)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder:\n\u001b[1;32m   1317\u001b[0m     variables, attrs \u001b[38;5;241m=\u001b[39m encoder(variables, attrs)\n\u001b[0;32m-> 1319\u001b[0m store\u001b[38;5;241m.\u001b[39mstore(variables, attrs, check_encoding, writer, unlimited_dims\u001b[38;5;241m=\u001b[39munlimited_dims)\n",
      "File \u001b[0;32m~/GitHub/lobSDM/.conda/lib/python3.11/site-packages/xarray/backends/common.py:274\u001b[0m, in \u001b[0;36mAbstractWritableDataStore.store\u001b[0;34m(self, variables, attributes, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     writer \u001b[38;5;241m=\u001b[39m ArrayWriter()\n\u001b[0;32m--> 274\u001b[0m variables, attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(variables, attributes)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_attributes(attributes)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_dimensions(variables, unlimited_dims\u001b[38;5;241m=\u001b[39munlimited_dims)\n",
      "File \u001b[0;32m~/GitHub/lobSDM/.conda/lib/python3.11/site-packages/xarray/backends/common.py:363\u001b[0m, in \u001b[0;36mWritableCFDataStore.encode\u001b[0;34m(self, variables, attributes)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, variables, attributes):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# All NetCDF files get CF encoded by default, without this attempting\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# to write times, for example, would fail.\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     variables, attributes \u001b[38;5;241m=\u001b[39m cf_encoder(variables, attributes)\n\u001b[1;32m    364\u001b[0m     variables \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_variable(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m variables\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    365\u001b[0m     attributes \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_attribute(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m attributes\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/GitHub/lobSDM/.conda/lib/python3.11/site-packages/xarray/conventions.py:779\u001b[0m, in \u001b[0;36mcf_encoder\u001b[0;34m(variables, attributes)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;66;03m# add encoding for time bounds variables if present.\u001b[39;00m\n\u001b[1;32m    777\u001b[0m _update_bounds_encoding(variables)\n\u001b[0;32m--> 779\u001b[0m new_vars \u001b[38;5;241m=\u001b[39m {k: encode_cf_variable(v, name\u001b[38;5;241m=\u001b[39mk) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m variables\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    781\u001b[0m \u001b[38;5;66;03m# Remove attrs from bounds variables (issue #2921)\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m new_vars\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/GitHub/lobSDM/.conda/lib/python3.11/site-packages/xarray/conventions.py:779\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;66;03m# add encoding for time bounds variables if present.\u001b[39;00m\n\u001b[1;32m    777\u001b[0m _update_bounds_encoding(variables)\n\u001b[0;32m--> 779\u001b[0m new_vars \u001b[38;5;241m=\u001b[39m {k: encode_cf_variable(v, name\u001b[38;5;241m=\u001b[39mk) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m variables\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    781\u001b[0m \u001b[38;5;66;03m# Remove attrs from bounds variables (issue #2921)\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m new_vars\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/GitHub/lobSDM/.conda/lib/python3.11/site-packages/xarray/conventions.py:190\u001b[0m, in \u001b[0;36mencode_cf_variable\u001b[0;34m(var, needs_copy, name)\u001b[0m\n\u001b[1;32m    178\u001b[0m ensure_not_multiindex(var, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m coder \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    181\u001b[0m     times\u001b[38;5;241m.\u001b[39mCFDatetimeCoder(),\n\u001b[1;32m    182\u001b[0m     times\u001b[38;5;241m.\u001b[39mCFTimedeltaCoder(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m     variables\u001b[38;5;241m.\u001b[39mBooleanCoder(),\n\u001b[1;32m    189\u001b[0m ]:\n\u001b[0;32m--> 190\u001b[0m     var \u001b[38;5;241m=\u001b[39m coder\u001b[38;5;241m.\u001b[39mencode(var, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# TODO(kmuehlbauer): check if ensure_dtype_not_object can be moved to backends:\u001b[39;00m\n\u001b[1;32m    193\u001b[0m var \u001b[38;5;241m=\u001b[39m ensure_dtype_not_object(var, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/GitHub/lobSDM/.conda/lib/python3.11/site-packages/xarray/coding/variables.py:235\u001b[0m, in \u001b[0;36mCFMaskCoder.encode\u001b[0;34m(self, variable, name)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m variable\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fv_exists \u001b[38;5;129;01mand\u001b[39;00m mv_exists \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m duck_array_ops\u001b[38;5;241m.\u001b[39mallclose_or_equiv(fv, mv):\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m has conflicting _FillValue (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) and missing_value (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Cannot encode data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fv_exists:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# Ensure _FillValue is cast to same dtype as data's\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_FillValue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mtype(fv)\n",
      "\u001b[0;31mValueError\u001b[0m: Variable 'time_bnds' has conflicting _FillValue (nan) and missing_value (1.0000000200408773e+20). Cannot encode data."
     ]
    }
   ],
   "source": [
    "import intake\n",
    "import xarray as xr\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from packaging.version import Version\n",
    "import numpy as np\n",
    "import gcsfs\n",
    "import fsspec\n",
    "# conda install -c conda-forge esmf esmpy\n",
    "import xesmf as xe\n",
    "import json\n",
    "import sys\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# A function to flag curvilinear grids as these have their own issues\n",
    "def is_curvilinear(ds):\n",
    "    lat_name = 'lat' if 'lat' in ds else 'latitude'\n",
    "    lon_name = 'lon' if 'lon' in ds else 'longitude'\n",
    "    \n",
    "    if lat_name in ds and lon_name in ds:\n",
    "        lat_dims = ds[lat_name].dims\n",
    "        lon_dims = ds[lon_name].dims\n",
    "        # Check if lat/lon have 2D dimensions\n",
    "        return len(lat_dims) == 2 and len(lon_dims) == 2\n",
    "    return False\n",
    "\n",
    "# A function to get lat/latitude or lon/longitude \n",
    "def get_lat_lon_names(ds):\n",
    "    \"\"\"\n",
    "    Dynamically detect the latitude and longitude variable names.\n",
    "    \"\"\"\n",
    "    lat_candidates = [\"lat\", \"latitude\", \"y\", \"j\"]\n",
    "    lon_candidates = [\"lon\", \"longitude\", \"x\", \"i\"]\n",
    "    \n",
    "    lat_name = next((name for name in lat_candidates if name in ds.dims), None)\n",
    "    lon_name = next((name for name in lon_candidates if name in ds.dims), None)\n",
    "    \n",
    "    if lat_name is None or lon_name is None:\n",
    "        raise KeyError(\"Could not find latitude and/or longitude variables in the dataset.\")\n",
    "    \n",
    "    return lat_name, lon_name\n",
    "\n",
    "# Function to plot data\n",
    "def plot_data(ds, variable, title=\"Quick Map\", cmap=\"viridis\", projection=ccrs.PlateCarree()):\n",
    "    if variable not in ds:\n",
    "        raise ValueError(f\"Variable '{variable}' not found in the dataset.\")\n",
    "    # Select a single time step if time is a dimension\n",
    "    if \"time\" in ds.dims:\n",
    "        data = ds[variable].isel(time=0)  # Plot the first time step\n",
    "    else:\n",
    "        data = ds[variable]\n",
    "    # Set up the map\n",
    "    fig, ax = plt.subplots(\n",
    "        subplot_kw={\"projection\": projection},\n",
    "        figsize=(10, 6)\n",
    "    )\n",
    "    ax.set_global()  # Set global extent (you can modify as needed)\n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=\":\")\n",
    "    ax.add_feature(cfeature.LAND, edgecolor=\"black\", facecolor=\"lightgray\", alpha=0.5)\n",
    "    # Plot the data\n",
    "    im = ax.pcolormesh(\n",
    "        ds[lon_name], ds[lat_name], data,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        cmap=cmap\n",
    "    )\n",
    "    # Add colorbar\n",
    "    cb = fig.colorbar(im, ax=ax, orientation=\"horizontal\", pad=0.05)\n",
    "    cb.set_label(variable)\n",
    "    # Title and gridlines\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.gridlines(draw_labels=True, linewidth=0.5, color=\"gray\", linestyle=\"--\", alpha=0.7)\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Path to your JSON file\n",
    "file_path = \"/Users/aallyn/GitHub/lobSDM/Code/cmip6_input.json\"\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(file_path, \"r\") as file:\n",
    "    input = json.load(file)\n",
    "\n",
    "# load metadata of the data in pangeo\n",
    "if len(input['metadata_csv'].strip()) > 0:\n",
    "    meta_data = pd.read_csv(input['metadata_csv'])\n",
    "    print(\"using local metadta from: \", input['metadata_csv'])\n",
    "else:\n",
    "    print(\"using metadta from https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv\")\n",
    "    meta_data = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\n",
    "\n",
    "# prepare the output directory\n",
    "out_dir = input['out_dir']\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "data_set=input['dataset']\n",
    "\n",
    "# check if the download all models option is invoked. If True, all unique models and ensemble members are obtained from metadata\n",
    "if input['download_all_models']:\n",
    "    models = meta_data['source_id'].unique()\n",
    "    all_models = True\n",
    "else:\n",
    "    models = data_set.keys()\n",
    "    all_models = False\n",
    "\n",
    "# check if the download all ensemble members option is invoked. If True, all unique models and ensemble members are obtained from metadata\n",
    "if input['download_all_members']:\n",
    "    ens_members = meta_data['member_id'].unique()\n",
    "    full_ensemble = True\n",
    "else:\n",
    "    ens_members = None\n",
    "    full_ensemble = False\n",
    "\n",
    "# check if regrid option is activated and create a lat lon array for target spatial grids\n",
    "if len(input['target_grid']) == 2:\n",
    "    regrid_data = True\n",
    "    tar_lat_int = input['target_grid'][0]\n",
    "    tar_lon_int = input['target_grid'][1]\n",
    "    n_lat = int(180./tar_lat_int)\n",
    "    n_lon = int(360./tar_lon_int)\n",
    "    lat_min = -90 + tar_lat_int/2.\n",
    "    lat_max = 90 - tar_lat_int/2.\n",
    "    lon_max = 360 - tar_lon_int/2.\n",
    "    lon_min = tar_lon_int/2.\n",
    "    new_lat = np.linspace(lat_min, lat_max, n_lat, endpoint=True)\n",
    "    new_lon = np.linspace(lon_min, lon_max, n_lon, endpoint=True)\n",
    "else:\n",
    "    regrid_data = False\n",
    "\n",
    "# loop through the experiments and dataset\n",
    "for experiment, info in input['experiments'].items():\n",
    "    start_year = info[0]\n",
    "    end_year = info[1]\n",
    "    # loop through the unique models\n",
    "    for model in models:\n",
    "        # get models if download_all_models/all_models is False\n",
    "        if not all_models:\n",
    "            print(model)\n",
    "            print(data_set[model])\n",
    "            src = data_set[model]['source_id']\n",
    "        else:\n",
    "            src = model\n",
    "\n",
    "        # get ensemble members if download_all_members/full_ensemble is False\n",
    "        if not full_ensemble:\n",
    "            ens_members = data_set[model]['ens_members']\n",
    "        # loop through the variables\n",
    "        for variable, table in input['variables'].items():\n",
    "            # loop through the ensemble\n",
    "            for variant in ens_members:\n",
    "                qry = \"table_id == '\" + table +\"' & variable_id == '\" + variable + \"' & experiment_id == '\" + experiment + \"' & source_id == '\" + src + \"' & member_id == '\" + variant + \"'\"\n",
    "                meta_data_sel = meta_data.query(qry)\n",
    "                \n",
    "                # Continue\n",
    "                if not meta_data_sel.empty:\n",
    "                    print(\"Trying to download: \")\n",
    "                    print(qry)\n",
    "                \n",
    "                    zstore = meta_data_sel.zstore.values[-1]\n",
    "                    # create a mutable-mapping-style interface to the store\n",
    "                    mapper = fsspec.get_mapper(zstore)\n",
    "                    # open it using xarray and zarr\n",
    "                    ds = xr.open_zarr(mapper, consolidated=True)\n",
    "                                        \n",
    "                    # create dataset with target grid\n",
    "                    print(\"Regridding irregular grid\")\n",
    "                    ds_out_target = xr.Dataset({'lat': (['lat'], new_lat), 'lon': (['lon'], new_lon),})\n",
    "                    regridder = xe.Regridder(ds, ds_out_target, input['regrid_method'])\n",
    "                    ds_regrid = regridder(ds)\n",
    "                    print(ds_regrid)\n",
    "                    \n",
    "                    # try:\n",
    "                    #     lat_name, lon_name = get_lat_lon_names(ds)\n",
    "                    # except KeyError as e:\n",
    "                    #     print(f\"Error: {e}\")\n",
    "                    #     sys.exit(1)\n",
    "                    \n",
    "                    lat_name = \"lat\"\n",
    "                    lon_name = \"lon\"\n",
    "                    \n",
    "                    # Perform subsetting\n",
    "                    ds_subset = ds_regrid.isel(\n",
    "                            lon=slice(-85, -50),\n",
    "                            lat=slice(255, 320)\n",
    "                    )\n",
    "                    \n",
    "                    ds_out = ds_subset.sel(time=slice(str(start_year), str(end_year)))\n",
    "                    print(\"Done subsetting regular grid\")\n",
    "                    # plot_data(ds_out_temp, variable, title=\"Quick Map\", cmap=\"viridis\", projection=ccrs.PlateCarree())\n",
    "                    \n",
    "                    # OLD STUFF       \n",
    "                    # curvilinear_flag = is_curvilinear(ds)\n",
    "                    # if curvilinear_flag:\n",
    "                    #     print(\"Detected curvilinear grid: Subsetting with lat/lon masks.\")\n",
    "                    #     # Subset using lat/lon bounds with 2D latitude/longitude variables\n",
    "                    #     if \"j\" in ds.dims and \"i\" in ds.dims:\n",
    "                    #         print(\"Trying j and i\")\n",
    "                    #         print(ds)\n",
    "                            \n",
    "                    #         # Checking as sometimes longitude seems weird\n",
    "                    #         min_check = ds.min(dim = \"i\")\n",
    "                            \n",
    "                    #         if (min_check < 0).any():\n",
    "                    #              # Manually subsetting\n",
    "                    #             ds_subset = ds.isel(\n",
    "                    #                 j=slice(255, 320),\n",
    "                    #                 i=slice(-85, -50)\n",
    "                    #             )\n",
    "                            \n",
    "                    #         else:\n",
    "                    #             # Manually subsetting\n",
    "                    #             ds_subset = ds.isel(\n",
    "                    #                 j=slice(255, 320),\n",
    "                    #                 i=slice(30, 60)\n",
    "                    #             )\n",
    "                        \n",
    "                    #     if \"x\" in ds.dims and \"y\" in ds.dims:\n",
    "                    #          ds_subset = ds.isel(\n",
    "                    #             x=slice(255, 320),\n",
    "                    #             y=slice(30, 60)\n",
    "                    #         )\n",
    "                             \n",
    "                    #     ds_out_temp = ds_subset.sel(time=slice(str(start_year), str(end_year)))\n",
    "                    #     print(\"Done subsetting irregular grid\")\n",
    "                    #     try:\n",
    "                    #         lat_name, lon_name = get_lat_lon_names(ds_out_temp)\n",
    "                    #     except KeyError as e:\n",
    "                    #         print(f\"Error: {e}\")\n",
    "                    #         sys.exit(1)\n",
    "                    #     # print(lat_name, lon_name)\n",
    "                    #     # plot_data(ds_out_temp, variable, title=\"Quick Map\", cmap=\"viridis\", projection=ccrs.PlateCarree())\n",
    "\n",
    "                    #     if regrid_data:\n",
    "                    #         print(\"Regridding irregular grid\")\n",
    "                    #         # create dataset with target grid\n",
    "                    #         ds_out_target = xr.Dataset({'lat': (['lat'], new_lat), 'lon': (['lon'], new_lon),})\n",
    "                            \n",
    "                    #         print(\"Dimensions:\", ds_out_temp.dims)\n",
    "                    #         print(\"Coordinates:\", ds_out_temp.coords)\n",
    "                            \n",
    "                    #         plt.plot(\n",
    "                    #             [ds_out_temp.j.min(), ds_out_temp.j.max(), ds_out_temp.j.max(), ds_out_temp.j.min(), ds_out_temp.j.min()],\n",
    "                    #             [ds_out_temp.i.min(), ds_out_temp.i.min(), ds_out_temp.i.max(), ds_out_temp.i.max(), ds_out_temp.i.min()],\n",
    "                    #             label=\"Source Grid\", color=\"blue\"\n",
    "                    #         )\n",
    "\n",
    "                    #         # Plot target grid bounds\n",
    "                    #         plt.plot(\n",
    "                    #             [ds_out_target.lon.min(), ds_out_target.lon.max(), ds_out_target.lon.max(), ds_out_target.lon.min(), ds_out_target.lon.min()],\n",
    "                    #             [ds_out_target.lat.min(), ds_out_target.lat.min(), ds_out_target.lat.max(), ds_out_target.lat.max(), ds_out_target.lat.min()],\n",
    "                    #             label=\"Target Grid\", color=\"red\"\n",
    "                    #         )\n",
    "\n",
    "                    #         plt.legend()\n",
    "                    #         plt.xlabel(\"Longitude\")\n",
    "                    #         plt.ylabel(\"Latitude\")\n",
    "                    #         plt.title(\"Spatial Extent Alignment\")\n",
    "                    #         plt.show()\n",
    "\n",
    "                    #         regridder = xe.Regridder(ds_out_temp, ds_out_target, input['regrid_method'])\n",
    "                    #         ds_out = regridder(ds_out_temp)\n",
    "                    #         print(ds_out)\n",
    "\n",
    "                    # else:\n",
    "                    #     print(\"Detected regular grid or indices: Subsetting with lat/lon or latitude/longitude.\")\n",
    "                    #     # Identify the correct coordinate names\n",
    "                    #     # lon_name = 'lon' if 'lon' in ds.coords and 'lon' in ds.variables else 'longitude'\n",
    "                    #     # lat_name = 'lat' if 'lat' in ds.coords and 'lat' in ds.variables else 'latitude'\n",
    "                    #     # Detect latitude and longitude variable names\n",
    "                    #     try:\n",
    "                    #         lat_name, lon_name = get_lat_lon_names(ds)\n",
    "                    #     except KeyError as e:\n",
    "                    #         print(f\"Error: {e}\")\n",
    "                    #         sys.exit(1)\n",
    "                                                \n",
    "                    #     # Perform subsetting\n",
    "                    #     ds_subset = ds.sel(\n",
    "                    #         **{\n",
    "                    #             lon_name: slice(lon_bounds[0], lon_bounds[1]),\n",
    "                    #             lat_name: slice(lat_bounds[0], lat_bounds[1]),\n",
    "                    #         }\n",
    "                    #     )\n",
    "                    #     ds_out_temp = ds_subset.sel(time=slice(str(start_year), str(end_year)))\n",
    "                    #     print(\"Done subsetting regular grid\")\n",
    "                        \n",
    "                    #     if regrid_data:\n",
    "                    #         # regrid if the option is invoked\n",
    "                    #         print(\"Regridding regular grid\")\n",
    "                    #         # create dataset with target grid\n",
    "                    #         ds_out_target = xr.Dataset({'lat': (['lat'], new_lat), 'lon': (['lon'], new_lon),})\n",
    "                    #         # Regrid\n",
    "                    #         regridder = xe.Regridder(ds_out_temp, ds_out_target, input['regrid_method'])\n",
    "                    #         ds_out = regridder(ds_out_temp)\n",
    "                        \n",
    "                    # create and save the output\n",
    "                    file_name_nc_out = '{var}_{exp}_{tab}_{sroc}_{vrnt}_{syr}_{eyr}_{t_lat}x{t_lon}.nc'.format(var=variable, exp=experiment, tab=table, sroc=src, vrnt=variant, syr=str(start_year), eyr=end_year, t_lat=str(tar_lat_int), t_lon=str(tar_lon_int))\n",
    "                    out_file_out = os.path.join(out_dir, file_name_nc_out)\n",
    "                    ds_out.to_netcdf(out_file_out)\n",
    "                    \n",
    "                    # close dataset\n",
    "                    ds_out.close()\n",
    "                    # ds_out_temp.close()\n",
    "                    print('saved regridded data: ', out_file_out)\n",
    "                    \n",
    "                    # close dataset\n",
    "                    ds.close()\n",
    "                else:\n",
    "                    print(f'Data not found for: {variable}')\n",
    "                    print(qry)\n",
    "                print ('---------------------------------------')\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
